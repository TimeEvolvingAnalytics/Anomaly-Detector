{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "import timeit\n",
    "import river\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## July Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "measurements = ['tdmp_bytes_created','tdmp_bytes_total','tdmp_packets_created','tdmp_packets_total']\n",
    "services = ['http','postgresql','ssh']\n",
    "files = ['2021-04-01_2021-05-01','2021-05-01_2021-06-01','2021-06-01_2021-07-01','2021-07-01_2021-08-01']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for m in measurements:\n",
    "    for service in services:\n",
    "        path = 'old_dataset/' + m + '/' + service + '/'\n",
    "        for file in files:\n",
    "            with open(path + file + '.csv', \"r\") as input_file:\n",
    "                rows = input_file.readlines()\n",
    "\n",
    "            with open(path + file + '_clean.csv', \"w\") as output_file:\n",
    "                for row in rows:\n",
    "                    if row[0] != '#':\n",
    "                        output_file.write(row) \n",
    "            try:\n",
    "                df = pd.read_csv(path + file + '_clean.csv',header=0)\n",
    "                df = df.drop(['Unnamed: 0', 'result','table','_start','_stop','_field','_measurement','service','host'], axis=1)\n",
    "                df.to_csv(path + file + '_clean.csv',index=False)\n",
    "            except:\n",
    "                continue\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## December Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "measurements = ['tdmp_bytes_created','tdmp_bytes_total','tdmp_packets_created','tdmp_packets_total']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for m in measurements:\n",
    "    path = 'anomaly_detector/dataset/' + m\n",
    "    with open(path + '.csv', \"r\") as input_file:\n",
    "        rows = input_file.readlines()\n",
    "\n",
    "    with open(path + '_clean.csv', \"w\") as output_file:\n",
    "        for row in rows:\n",
    "            if row[0] != '#':\n",
    "                output_file.write(row) \n",
    "    try:\n",
    "        df = pd.read_csv(path + '_clean.csv',header=0)\n",
    "        df = df.drop(['Unnamed: 0','result','table','_start','_stop','_field','_measurement','host'], axis=1)\n",
    "        df['hash_table'] = df.apply(lambda row: row['dst'] + '_' + row['dstp'] + '_' + row['proto'] + '_' + row['service'] + '_' + row['src'] + '_' + row['srcp'] + '_' + row['url'],axis=1)\n",
    "        df.to_csv(path + '.csv',index=False)\n",
    "    except:\n",
    "        continue\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## July Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "measurements = ['tdmp_bytes_created','tdmp_bytes_total','tdmp_packets_created','tdmp_packets_total']\n",
    "services = ['http','postgresql','ssh']\n",
    "files = ['2021-04-01_2021-05-01_clean','2021-05-01_2021-06-01_clean','2021-06-01_2021-07-01_clean','2021-07-01_2021-08-01_clean']\n",
    "\n",
    "for m in measurements:\n",
    "    for service in services:\n",
    "        title = m + ' ' + service        \n",
    "        path = 'old_dataset/' + m + '/' + service + '/'\n",
    "        df = pd.DataFrame()\n",
    "        for file in files:            \n",
    "            f = pd.read_csv(path + file + '.csv',header=0).sort_values(by='_time',ascending=True)                \n",
    "            df = pd.concat([df,f],axis=0,ignore_index=True)                \n",
    "            fig = plt.figure(figsize=(30, 15))\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.title(file)\n",
    "            plt.plot(df[\"_time\"], df[\"_value\"])\n",
    "            plt.show()\n",
    "            #fig.savefig('old_dataset/' + m + '/' + service + '/plots/' + file + '.png', dpi=fig.dpi)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## December Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "measurements = ['tdmp_bytes_created','tdmp_bytes_total','tdmp_packets_created','tdmp_packets_total']\n",
    "\n",
    "for m in measurements:\n",
    "    path = 'anomaly_detector/dataset/' + m\n",
    "    df = pd.DataFrame()          \n",
    "    f = pd.read_csv(path + '.csv',header=0).sort_values(by='_time',ascending=True)                \n",
    "    df = pd.concat([df,f],axis=0,ignore_index=True)                \n",
    "    \n",
    "    fig = plt.figure(figsize=(40, 15))        \n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax1.set_title(m)\n",
    "    ax2.set_title(m + \" differenced\")\n",
    "    ax1.plot(df[\"_time\"], df[\"_value\"])\n",
    "    ax1.tick_params(axis='x', rotation=90)\n",
    "    ax2.plot(df[\"_time\"], df[\"_value\"].diff())\n",
    "    ax2.tick_params(axis='x', rotation=90)\n",
    "    plt.show()\n",
    "    fig.savefig('plots/' + m + '.png', dpi=fig.dpi)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z-Score New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isAnomaly(value,mean,std,tolerance):\n",
    "    if float(std) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        zscore = (value - mean) / std\n",
    "        if zscore > tolerance:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "measurements = ['tdmp_bytes_created','tdmp_bytes_total','tdmp_packets_created','tdmp_packets_total']\n",
    "\n",
    "for m in measurements:\n",
    "    path = 'anomaly_detector/dataset/' + m\n",
    "    df = pd.DataFrame()          \n",
    "    f = pd.read_csv(path + '.csv',header=0).sort_values(by='_time',ascending=True)                \n",
    "    df = pd.concat([df,f],axis=0,ignore_index=True) \n",
    "    df['_value_diff'] = df['_value'].diff()\n",
    "    df['_value_diff_squared'] = df['_value_diff'].pow(2)\n",
    "    stats = pd.DataFrame()    \n",
    "    stats['avg_diff'] = df.groupby('hash_table')['_value_diff'].mean()\n",
    "    stats['std_diff'] = df.groupby('hash_table')['_value_diff'].std()  \n",
    "    stats['cnt_diff'] = df.groupby('hash_table')['_value_diff'].size()\n",
    "    stats['ss_diff'] = df.groupby('hash_table')['_value_diff_squared'].sum()\n",
    "    stats_dict = stats.T.to_dict('list')\n",
    "    df['anomaly_diff'] = df.apply(lambda row: isAnomaly(row['_value_diff'],stats_dict[row['hash_table']][0],stats_dict[row['hash_table']][1],3),axis=1)\n",
    "    df.to_csv(path + '.csv',index=False)\n",
    "    stats.reset_index().to_csv('anomaly_detector/dataset/stats/' + m + '.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for m in measurements:\n",
    "    path = 'anomaly_detector/dataset/' + m\n",
    "    f = pd.read_csv(path + '.csv',header=0)\n",
    "    print(m)\n",
    "    print('#anomalies_diff: ' + str(f.loc[f['anomaly_diff']==1].shape[0]), '#normal_diff: ' + str(f.loc[f['anomaly_diff']==0].shape[0]))\n",
    "    print()\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set InfluxDB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import influxdb_client\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"riot\"\n",
    "org = \"polimi\"\n",
    "token = \"d2VsY29tZQ==\"\n",
    "url=\"http://35.152.63.133:8086\"\n",
    "\n",
    "client = influxdb_client.InfluxDBClient(\n",
    "   url=url,\n",
    "   token=token,\n",
    "   org=org\n",
    ")\n",
    "\n",
    "query_api = client.query_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get parameters from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import requests\n",
    "from requests import Request, Session\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAuth(username,method,uri):\n",
    "    #auth1 = \"user@host.com:\" + username + ':' + password\n",
    "    #hash_object = hashlib.md5(auth1.encode())\n",
    "    #hash1 = hash_object.hexdigest().upper()\n",
    "    hash1 = '7CC384C2A10CEA62FB2A37CFDA222C04'\n",
    "    \n",
    "    now = '{:8X}'.format(int(time.time()))    \n",
    "    POOL = \"ABCDEF0123456789\"\n",
    "    nonce_list = []\n",
    "    for i in range(0,24):\n",
    "        nonce_list.append(POOL[random.randint(0,len(POOL)-1)])\n",
    "    nonce_str = ''.join(nonce_list)\n",
    "    nonce = now + nonce_str\n",
    "    \n",
    "    auth2 = method + ':' + uri\n",
    "    hash_object = hashlib.md5(auth2.encode())\n",
    "    hash2 = hash_object.hexdigest().upper()\n",
    "    \n",
    "    auth3 = hash1 + ':' + nonce + ':' + hash2\n",
    "    hash_object = hashlib.md5(auth3.encode())\n",
    "    authority = hash_object.hexdigest().upper()\n",
    "    \n",
    "    return \"oasis username=\\\"\" + \"service\" + \"\\\", nonce=\\\"\" + nonce + \"\\\", authority=\\\"\" + authority + \"\\\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Authorization': 'oasis username=\"service\", nonce=\"61BC5CE44BB38A2C2E3D965D5D1BC4AD\", authority=\"9868BCA6BBF4FE8D6E4DD003BECAD960\"', 'Content-Type': 'application/json', 'Cache-Control': 'no-cache'}\n"
     ]
    }
   ],
   "source": [
    "method = 'GET'\n",
    "uri = '/global' # use '/global' to access the variables, '/auth' only for the authentication\n",
    "username = 'service'\n",
    "BASE_URI = 'https://demo.riotsecure.io:6443'\n",
    "headers = {\n",
    "    'Authorization': createAuth(username,method,uri), \n",
    "    'Content-Type': \"application/json\",\n",
    "    'Cache-Control': \"no-cache\"\n",
    "}\n",
    "print(headers)\n",
    "body = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The request was a success!\n",
      "The tolerance is 3\n",
      "The frequency is 24\n"
     ]
    }
   ],
   "source": [
    "if method == \"GET\":\n",
    "    _uri = uri + \"?expand\"\n",
    "    response = requests.get(BASE_URI + _uri, headers=headers)\n",
    "elif method == \"POST\":\n",
    "    response = requests.post(BASE_URI + _uri, headers=headers, json=body)\n",
    "\n",
    "\n",
    "tolerance = 3\n",
    "frequency = '-24h'    \n",
    "    \n",
    "if response.status_code == 200:\n",
    "    #in console, print in stderror\n",
    "    print(\"The request was a success!\")\n",
    "    print('The tolerance is ' + str(response.json()['keys']['net_anomoly']['tolerance']))\n",
    "    print('The frequency is ' + str(response.json()['keys']['net_anomoly']['frequency']))\n",
    "    #bisogna fare un controllo??\n",
    "    tolerance = response.json()['keys']['net_anomoly']['tolerance']\n",
    "    frequency = '-' + str(response.json()['keys']['net_anomoly']['frequency']) + 'h'\n",
    "elif response.status_code == 404:\n",
    "    print(\"Error 404\")\n",
    "elif response.status_code == 401:\n",
    "    print(\"401: Unauthorized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check new entries New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** New dstp ***\n",
      "*** New service ***\n",
      "*** New srcp ***\n"
     ]
    }
   ],
   "source": [
    "tags = ['dst','dstp','service','src','srcp','url']\n",
    "\n",
    "for tag in tags:\n",
    "    path = 'anomaly_detector/dataset/lists/' + tag\n",
    "    df = pd.read_csv(path + '.csv',header=0)\n",
    "    l = df['_value'].tolist()\n",
    "    #eseguito giornalieralmente, come parametro\n",
    "    q = 'import \"influxdata/influxdb/schema\"\\\n",
    "        schema.tagValues(\\\n",
    "        bucket: \"riot\",\\\n",
    "        tag: \"' + tag + '\",\\\n",
    "        start: ' + frequency + ')'\n",
    "    #system.exit(0)\n",
    "    result = query_api.query(org=org, query=q)\n",
    "    results = []\n",
    "    for table in result:\n",
    "        for record in table.records:\n",
    "            results.append((record.get_value()))\n",
    "    diffs = list(set(results) - set(l))\n",
    "    if len(diffs) != 0:\n",
    "        #call API\n",
    "        print('*** New ' + tag + ' ***')\n",
    "        for diff in diffs:\n",
    "            #print(diff)\n",
    "            new_row = {'_value': diff}\n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "    df.to_csv(path + '.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check new Anomalies z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkNewHash(hash_table,diff_hash):\n",
    "    if hash_table in diff_hash:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateStatsDict(stats_dict,df):\n",
    "    actual_hash = list(stats_dict.keys())\n",
    "    new_hash = df['hash_table'].unique().tolist()\n",
    "    diff_hash = list(set(new_hash) - set(actual_hash))    \n",
    "    stats = pd.DataFrame()\n",
    "    stats['avg_diff'] = df.groupby('hash_table')['_value_diff'].mean()\n",
    "    stats['std_diff'] = df.groupby('hash_table')['_value_diff'].std()  \n",
    "    stats['cnt_diff'] = df.groupby('hash_table')['_value_diff'].size()\n",
    "    stats['ss_diff'] = df.groupby('hash_table')['_value_diff_squared'].sum()\n",
    "    stats = stats[stats.index.isin(diff_hash)]\n",
    "    new_stats_dict = stats.T.to_dict('list')\n",
    "    stats_dict.update(new_stats_dict)\n",
    "    df['new_hash'] = df.apply(lambda row: checkNewHash(row['hash_table'],diff_hash),axis=1)\n",
    "    return stats_dict,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Anomalies in tdmp_bytes_created ***\n",
      "*** Anomalies in tdmp_bytes_total ***\n",
      "*** Anomalies in tdmp_packets_created ***\n",
      "*** Anomalies in tdmp_packets_total ***\n"
     ]
    }
   ],
   "source": [
    "measurements = ['tdmp_bytes_created','tdmp_bytes_total','tdmp_packets_created','tdmp_packets_total']\n",
    "\n",
    "for m in measurements:\n",
    "    path = 'anomaly_detector/dataset/stats/' + m\n",
    "    stats_dict = pd.read_csv(path + '.csv',header=0).set_index('hash_table').T.to_dict('list')\n",
    "    s = pd.read_csv('anomaly_detector/dataset/lists/service.csv',header=0)\n",
    "    services = s['_value'].tolist()\n",
    "    f = ''\n",
    "    for service in services:\n",
    "        if service == services[-1]:\n",
    "            f += 'r[\"service\"] == \"' + service + '\"'\n",
    "        else:\n",
    "            f += 'r[\"service\"] == \"' + service + '\" or '\n",
    "    #eseguito giornalieralmente, come parametro\n",
    "    q = 'from(bucket: \"' + bucket + '\")\\\n",
    "      |> range(start: ' + frequency + ')\\\n",
    "      |> filter(fn: (r) => r[\"_measurement\"] == \"' + m + '\")\\\n",
    "      |> filter(fn: (r) => ' + f + ')'\n",
    "    result = query_api.query(org=org, query=q)\n",
    "    results = []\n",
    "    for table in result:\n",
    "        for record in table.records:\n",
    "            results.append([record.values.get(\"_time\"),record.values.get(\"dst\"),record.values.get(\"dstp\"),record.values.get(\"proto\"),record.values.get(\"service\"),record.values.get(\"src\"),record.values.get(\"srcp\"),record.values.get(\"url\"), record.get_value()])\n",
    "    df = pd.DataFrame(results,columns=['_time','dst', 'dstp', 'proto','service','src','srcp','url','_value'])\n",
    "    df['_value_diff'] = df['_value'].diff()\n",
    "    df['_value_diff_squared'] = df['_value_diff'].pow(2)\n",
    "    df['hash_table'] = df.apply(lambda row: row['dst'] + '_' + row['dstp'] + '_' + row['proto'] + '_' + row['service'] + '_' + row['src'] + '_' + row['srcp'] + '_' + row['url'],axis=1)\n",
    "    stats_dict,df = updateStatsDict(stats_dict,df)\n",
    "    df['anomaly_diff'] = df.apply(lambda row: isAnomaly(row['_value_diff'],stats_dict[row['hash_table']][0],stats_dict[row['hash_table']][1],tolerance),axis=1)\n",
    "    anomalies = df.loc[df['anomaly_diff'] == 1]\n",
    "    if anomalies.shape[0] != 0:\n",
    "        #call API POST\n",
    "        print('*** Anomalies in ' + m + ' ***')        \n",
    "        anomalies_drop = anomalies.drop(['_value_diff_squared','new_hash','anomaly_diff'], axis=1)\n",
    "        anomalies_drop.index.names = ['uuid']\n",
    "        if os.stat('anomaly_detector/dataset/anomalies/' + m + '.csv').st_size == 0:            \n",
    "            anomalies_drop.to_csv('anomaly_detector/dataset/anomalies/' + m + '.csv',index=True)\n",
    "        else:        \n",
    "            existing_anomalies = pd.read_csv('anomaly_detector/dataset/anomalies/' + m + '.csv').append(anomalies_drop, ignore_index = True)\n",
    "            existing_anomalies.to_csv('anomaly_detector/dataset/anomalies/' + m + '.csv',index=True)\n",
    "    normalities = df.loc[(df['anomaly_diff'] == 0) & (df['new_hash'] == 0)]\n",
    "    if normalities.shape[0] != 0:\n",
    "        for index, row in normalities.iterrows():\n",
    "            stat = stats_dict[row['hash_table']]\n",
    "            #avg += (value-avg)/n \n",
    "            stat[0] += (row['_value_diff'] - stat[0]) / stat[2]\n",
    "            stat[3] += math.pow(row['_value_diff'],2)\n",
    "            stat[2] += 1\n",
    "            #std = sqrt((1/(n-1)*(ss-(avg.pow(2)/n))))\n",
    "            stat[1] = math.sqrt((1 / (stat[2]-1)) * (stat[3] - (math.pow(stat[0],2) / stat[2])))\n",
    "            stats_dict[row['hash_table']] = stat\n",
    "    pd.DataFrame.from_dict(stats_dict,orient='index').reset_index().rename(columns={'index':'hash_table', 0:'avg_diff', 1:'std_diff', 2:'cnt_diff', 3:'ss_diff'}).to_csv(path + '.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics to use\n",
    "\n",
    "# più grande è, meglio è\n",
    "def em(t, t_max, volume_support, s_unif, s_X, n_generated):\n",
    "    EM_t = np.zeros(t.shape[0])\n",
    "    n_samples = s_X.shape[0]\n",
    "    s_X_unique = np.unique(s_X)\n",
    "    EM_t[0] = 1.\n",
    "    for u in s_X_unique:\n",
    "        # if (s_unif >= u).sum() > n_generated / 1000:\n",
    "        EM_t = np.maximum(EM_t, 1. / n_samples * (s_X > u).sum() -\n",
    "                          t * (s_unif > u).sum() / n_generated\n",
    "                          * volume_support)\n",
    "    amax = np.argmax(EM_t <= t_max) + 1\n",
    "    if amax == 1:\n",
    "        print ('\\n failed to achieve t_max \\n')\n",
    "        amax = -1\n",
    "    AUC = auc(t[:amax], EM_t[:amax])\n",
    "    return AUC, EM_t, amax\n",
    "\n",
    "\n",
    "# più piccolo è, meglio è\n",
    "def mv(axis_alpha, volume_support, s_unif, s_X, n_generated):\n",
    "    n_samples = s_X.shape[0]\n",
    "    s_X_argsort = s_X.argsort()\n",
    "    mass = 0\n",
    "    cpt = 0\n",
    "    u = s_X[s_X_argsort[-1]]\n",
    "    mv = np.zeros(axis_alpha.shape[0])\n",
    "    for i in range(axis_alpha.shape[0]):\n",
    "        # pdb.set_trace()\n",
    "        while mass < axis_alpha[i]:\n",
    "            cpt += 1\n",
    "            u = s_X[s_X_argsort[-cpt]]\n",
    "            mass = 1. / n_samples * cpt  # sum(s_X > u)\n",
    "        mv[i] = float((s_unif >= u).sum()) / n_generated * volume_support\n",
    "    return auc(axis_alpha, mv), mv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_generated = 100000\n",
    "alpha_min = 0.9\n",
    "alpha_max = 0.999\n",
    "t_max = 0.9\n",
    "ocsvm_max_train = 10000\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df):    \n",
    "    df['_time'] = pd.to_datetime(df['_time']).astype(np.int64) / int(1e6)\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    ohencDf = df.copy()\n",
    "    encoding = pd.DataFrame(encoder.fit_transform(ohencDf[[\"dst\",\"dstp\",\"proto\",\"src\",\"srcp\",\"url\"]]).toarray())\n",
    "    ohencDf = ohencDf.drop([\"dst\",\"dstp\",\"proto\",\"src\",\"srcp\",\"url\"],axis=1)\n",
    "    ohencDf = ohencDf.join(encoding)\n",
    "    n_train = int(ohencDf.shape[0] * 0.8)\n",
    "    train = np.array(ohencDf.head(n_train))\n",
    "    test = (ohencDf.tail(ohencDf.shape[0]-n_train))\n",
    "    return ohencDf,np.array(train),np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_parameters(df):\n",
    "    lim_inf = np.array(df).min(axis=0)\n",
    "    lim_sup = np.array(df).max(axis=0)\n",
    "    volume_support = (lim_sup - lim_inf).prod()\n",
    "    volume_support = volume_support if volume_support > 1 else 1\n",
    "    t = np.arange(0, 100 / volume_support, 0.01 / volume_support)\n",
    "    axis_alpha = np.arange(alpha_min, alpha_max, 0.0001)\n",
    "    unif = np.random.uniform(lim_inf, lim_sup,size=(n_generated, df.shape[1]))\n",
    "    return t,volume_support,axis_alpha,unif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,train,test,unif,reshape=False):\n",
    "    model.fit(train)\n",
    "    s_X_model = model.decision_function(test)\n",
    "    s_unif_model = model.decision_function(unif)\n",
    "    if reshape:\n",
    "        #model.fit(train[:min(ocsvm_max_train, n_samples_train - 1)])\n",
    "        s_X_model = s_X_model.reshape(1, -1)[0]\n",
    "        s_unif_model = s_unif_model.reshape(1, -1)[0]                           \n",
    "    return s_X_model,s_unif_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(t,volume_support,s_unif_model,s_X_model,axis_alpha):\n",
    "    auc_em_model,em_model,amax_model = em(t, t_max,volume_support,s_unif_model,s_X_model,n_generated)\n",
    "    auc_mv_model,mv_model = mv(axis_alpha,volume_support,s_unif_model, s_X_model,n_generated)\n",
    "    return auc_em_model,em_model,amax_model,auc_mv_model,mv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(path,title,t,amax,em_iforest,mv_iforest,auc_em_iforest,auc_mv_iforest,em_lof,mv_lof,auc_em_lof,auc_mv_lof,em_ocsvm,mv_ocsvm,auc_em_ocsvm,auc_mv_ocsvm,axis_alpha):\n",
    "    plt.clf()    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(10, 5))\n",
    "    fig.suptitle(title,fontsize=25,y=1.05)    \n",
    "\n",
    "    ax1.plot(t[:amax], em_iforest[:amax], lw=1,label='%s (em_score = %0.3e)'% ('iforest', auc_em_iforest))\n",
    "    ax1.plot(t[:amax], em_lof[:amax], lw=1,label='%s (em-score = %0.3e)'% ('lof', auc_em_lof))\n",
    "    ax1.plot(t[:amax], em_ocsvm[:amax], lw=1,label='%s (em-score = %0.3e)'% ('ocsvm', auc_em_ocsvm))\n",
    "    ax1.set_xlabel('t',fontsize=20)\n",
    "    ax1.set_ylabel('EM(t)',fontsize=20)\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.set_title('Excess-Mass curve', fontsize=20)\n",
    "    ax1.legend(loc=\"upper center\",bbox_to_anchor=(0.5,-0.15),fancybox=True)\n",
    "\n",
    "    ax2.plot(axis_alpha, mv_iforest, lw=1,label='%s (mv-score = %0.3e)'% ('iforest', auc_mv_iforest))\n",
    "    ax2.plot(axis_alpha, mv_lof, lw=1,label='%s (mv-score = %0.3e)'% ('lof', auc_mv_lof))\n",
    "    ax2.plot(axis_alpha, mv_ocsvm, lw=1,label='%s (mv-score = %0.3e)'% ('ocsvm', auc_mv_ocsvm))    \n",
    "    ax2.set_xlabel('alpha', fontsize=20)\n",
    "    ax2.set_ylabel('MV(alpha)', fontsize=20)\n",
    "    ax2.set_title('Mass-Volume Curve', fontsize=20)\n",
    "    ax2.legend(loc=\"upper center\",bbox_to_anchor=(0.5,-0.15),fancybox=True)\n",
    "    \n",
    "    fig.subplots_adjust(wspace=0.4)\n",
    "    #plt.show()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + 'EM_MV_plot.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['2021-04-01_2021-05-01_clean','2021-05-01_2021-06-01_clean','2021-06-01_2021-07-01_clean','2021-07-01_2021-08-01_clean']\n",
    "\n",
    "for value in values:\n",
    "    for service in services:\n",
    "        title = value + ' ' + service\n",
    "        print(title)\n",
    "        path = 'dataset/' + value + '/' + service + '/'\n",
    "        df = pd.DataFrame()\n",
    "        for file in files:            \n",
    "            try:\n",
    "                f = pd.read_csv(path + file + '.csv',header=0).sort_values(by='_time',ascending=True)                \n",
    "                df = pd.concat([df,f],axis=0,ignore_index=True)                \n",
    "            except:\n",
    "                continue        \n",
    "        df_tot,df_train,df_test = split_df(df)\n",
    "        t,volume_support,axis_alpha,unif = calculate_parameters(df_tot)\n",
    "        start = timeit.default_timer()\n",
    "        s_X_iforest,s_unif_iforest = evaluate_model(IsolationForest(),df_train,df_test,unif)\n",
    "        s_X_lof,s_unif_lof = evaluate_model(LocalOutlierFactor(n_neighbors=20,novelty=True),df_train,df_test,unif)\n",
    "        s_X_ocsvm,s_unif_ocsvm = evaluate_model(OneClassSVM(),df_train,df_test,unif,True)\n",
    "        stop = timeit.default_timer()\n",
    "        print('Time: ', stop - start)\n",
    "        auc_em_iforest,em_iforest,amax_iforest,auc_mv_iforest,mv_iforest = calculate_metrics(t,volume_support,s_unif_iforest,s_X_iforest,axis_alpha)\n",
    "        auc_em_lof,em_lof,amax_lof,auc_mv_lof,mv_lof = calculate_metrics(t,volume_support,s_unif_lof,s_X_lof,axis_alpha)\n",
    "        auc_em_ocsvm,em_ocsvm,amax_ocsvm,auc_mv_ocsvm,mv_ocsvm = calculate_metrics(t,volume_support,s_unif_ocsvm,s_X_ocsvm,axis_alpha)\n",
    "        if amax_iforest == -1 or amax_lof == -1 or amax_ocsvm == -1:\n",
    "            amax = -1\n",
    "        else:\n",
    "            amax = max(amax_iforest, amax_lof, amax_ocsvm)\n",
    "        plot(path,title,t,amax,em_iforest,mv_iforest,auc_em_iforest,auc_mv_iforest,em_lof,mv_lof,auc_em_lof,auc_mv_lof,em_ocsvm,mv_ocsvm,auc_em_ocsvm,auc_mv_ocsvm,axis_alpha)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Anomaly Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import anomaly\n",
    "from river import compose\n",
    "from river import metrics\n",
    "from river import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = ['tdmp_bytes_created','tdmp_bytes_total','tdmp_packets_created','tdmp_packets_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROCAUC: 0.025045 tdmp_bytes_created\n",
      "ROCAUC: 0.553355 tdmp_bytes_total\n",
      "ROCAUC: 0.025045 tdmp_packets_created\n",
      "ROCAUC: 0.698017 tdmp_packets_total\n"
     ]
    }
   ],
   "source": [
    "for m in measurements:\n",
    "    model = compose.Pipeline(\n",
    "        #preprocessing.MinMaxScaler(),\n",
    "        #anomaly.OneClassSVM(nu=0.2))\n",
    "        anomaly.HalfSpaceTrees(seed=42))\n",
    "    train = pd.read_csv('anomaly_detector/dataset/' + m + '.csv')\n",
    "    y = train['anomaly_diff']\n",
    "    train = train.drop(['_time','_value','dstp','hash_table','_value_diff_squared','anomaly_diff'], axis=1)\n",
    "    for column in train:\n",
    "        if column != '_value_diff':\n",
    "            le = LabelEncoder()\n",
    "            train[column] = le.fit_transform(train[column])\n",
    "    auc = metrics.ROCAUC()\n",
    "    for index, row in train.iterrows():\n",
    "        row = row.fillna(0)\n",
    "        x = row.to_dict()\n",
    "        score = model.score_one(x)\n",
    "        model = model.learn_one(x)\n",
    "        auc = auc.update(y[index], score)\n",
    "    print(str(auc) + ' ' + m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROCAUC: 0.025045 tdmp_bytes_created\n",
      "ROCAUC: 0.669099 tdmp_bytes_total\n",
      "ROCAUC: 0.025045 tdmp_packets_created\n",
      "ROCAUC: 0.627969 tdmp_packets_total\n"
     ]
    }
   ],
   "source": [
    "for m in measurements:\n",
    "    auc = metrics.ROCAUC()\n",
    "    model = compose.Pipeline(\n",
    "        preprocessing.MinMaxScaler(),\n",
    "        anomaly.HalfSpaceTrees(seed=42))\n",
    "    train = pd.read_csv('anomaly_detector/dataset/' + m + '.csv')\n",
    "    y = train['anomaly_diff']\n",
    "    train = train['_value_diff']\n",
    "    for index, row in train.iteritems():\n",
    "        x = {'x': row}\n",
    "        score = model.score_one(x)\n",
    "        model = model.learn_one(x)\n",
    "        auc = auc.update(y[index], score)\n",
    "    print(str(auc) + ' ' + m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
